{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Workflow of MetaRF - Tutorial\n",
        "\n",
        "> *This ipynb introduces the whole work flow of MetaRF. The code includes three modules, Data preprocessing, Model training, Model fine-tuning and testing. For each module, there exists some submodules, which are listed below. We provide introduction under each submodule (at next section) to help users understand the workflow and run the code.*\n",
        "### Data preprocessing\n",
        "*   Random forest\n",
        "*   Dimension reduction\n",
        "\n",
        "### Model training\n",
        "*   Installation\n",
        "*   Model definition\n",
        "*   Training implementation\n",
        "*   Training model\n",
        "*   Validation, model selection and model saving\n",
        "\n",
        "### Model fine-tuning and testing\n",
        "*   Loading model\n",
        "*   Fine-tuning and printing experiment results"
      ],
      "metadata": {
        "id": "e3pIfJKrDzWe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code"
      ],
      "metadata": {
        "id": "TPHRYDT5UvoL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preprocessing"
      ],
      "metadata": {
        "id": "e9pUdzwFFdh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random forest\n",
        "\n",
        "> *This step performs random forest. We take it out as a seperate module to make the code structure more clearly.*"
      ],
      "metadata": {
        "id": "Mh-e5CkmFn91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Parameters (can be changed according to experiment setting)\n",
        "\n",
        "BASE_DIR = os.getcwd() # The current working directory\n",
        "input_path = os.path.join(BASE_DIR, 'data/Original_Data_Buchwald_Hartwig_HTE.csv') # Relative path\n",
        "output_path = os.path.join(BASE_DIR, 'data/Data_After_Preprocessing_Buchwald_Hartwig_HTE.csv')\n",
        "\n",
        "data = pd.read_csv(input_path) \n",
        "reagent = 'additive_number' \n",
        "\n",
        "list_train = [1,2,3,4]\n",
        "list_test = [5,6,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]\n",
        "\n",
        "dft_num = 120 # Number of the original reaction encoding\n",
        "\n",
        "train = data[data[reagent].isin(list_train)].sample(frac=1)\n",
        "test = data[data[reagent].isin(list_test)]\n",
        "  \n",
        "X_test = test.iloc[:,0:dft_num]\n",
        "X_train = train.iloc[:,0:dft_num]\n",
        "y_test = test.iloc[:,dft_num:dft_num+1].values\n",
        "y_train = train.iloc[:,dft_num:dft_num+1].values \n",
        "\n",
        "regressor_student = RandomForestRegressor(n_estimators=200,max_features=61)\n",
        "regressor_student.fit(X_train, y_train) \n",
        "\n",
        "y_pred = regressor_student.predict(X_test)  \n",
        "\n",
        "per_tree_pred = np.array([tree.predict(data.iloc[:,0:dft_num].values) for tree in regressor_student.estimators_])\n",
        "per_tree_pred_df = pd.DataFrame(per_tree_pred).T\n",
        "\n",
        "rf_result = per_tree_pred_df\n",
        "rf_result[reagent] = data[reagent]\n",
        "rf_result['yield'] = data['yield_dft']\n",
        "data_all = pd.concat([data.iloc[:,0:dft_num],rf_result],axis=1)\n",
        "feature_num = dft_num + 200"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AQFBcE6wtYR",
        "outputId": "796a48b5-8ec2-43fa-8bc6-1970c05dfa9e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-23ca219b8b5b>:34: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  regressor_student.fit(X_train, y_train)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dimension reduction\n",
        "> *This step applies TSNE to reduce the dimension of reaction data.*"
      ],
      "metadata": {
        "id": "dC7otaEFFsJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_all_copy = data_all.copy(deep=True)\n",
        "feature = data_all_copy.iloc[:,0:feature_num]\n",
        "feature = feature.values\n",
        "\n",
        "X = feature\n",
        "tsne = TSNE(n_components=2)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "x_min, x_max = X_tsne.min(0), X_tsne.max(0)\n",
        "X_norm = (X_tsne - x_min) / (x_max - x_min)  # 归一化\n",
        "\n",
        "tsne = pd.DataFrame(X_norm)\n",
        "tsne.columns = ['tsne_1','tsne_2']\n",
        "\n",
        "data_all_copy = pd.concat([data_all_copy, tsne], axis=1)\n",
        "#data_all_copy.to_csv(os.path.join(BASE_DIR, output_path),index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8A0nx3aFusj",
        "outputId": "d08aa139-eb53-42b6-dc40-e7ecd428f143"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/manifold/_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/manifold/_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model training"
      ],
      "metadata": {
        "id": "vxIj-_PxF3I7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Installation\n",
        "> *This step installs the necessary pachages.*"
      ],
      "metadata": {
        "id": "thnaUSJQJTT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kennard-stone"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiEk8AcrJVV7",
        "outputId": "883756d0-6e7c-4419-a300-0100a7c30af5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kennard-stone in /usr/local/lib/python3.8/dist-packages (1.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from kennard-stone) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from kennard-stone) (1.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from kennard-stone) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->kennard-stone) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->kennard-stone) (2022.7)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->kennard-stone) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->kennard-stone) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->kennard-stone) (1.7.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->kennard-stone) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow-gpu==2.9.0"
      ],
      "metadata": {
        "id": "t9b5fg1zJZlS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model definition\n",
        "> *This step defines the model structure.*"
      ],
      "metadata": {
        "id": "7LB_8SrEGGZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow.keras.backend as keras_backend\n",
        "from sklearn import metrics\n",
        "from pandas.core.frame import DataFrame\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import kennard_stone as ks\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "#BASE_DIR = os.getcwd() # The current working directory\n",
        "#input_path = os.path.join(BASE_DIR, 'data/Data_After_Preprocessing_Buchwald_Hartwig_HTE.csv') # Relative path\n",
        "#output_path = os.path.join(BASE_DIR, 'model/model_trained.h5') \n",
        "\n",
        "#data_all_copy = pd.read_csv(input_path) # Read the csv file with relative path\n",
        "\n",
        "list_train = [1,2,3,4]\n",
        "list_val = [5]\n",
        "list_test = [6,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]\n",
        "\n",
        "train_all = data_all[data_all[reagent].isin(list_train)].sample(frac=1)\n",
        "test_all = data_all[data_all[reagent].isin(list_test)]\n",
        "\n",
        "class Model(keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.hidden1 = keras.layers.Dense(40, input_shape=(feature_num,))\n",
        "        self.hidden2 = keras.layers.Dense(40)\n",
        "        self.out = keras.layers.Dense(1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = keras.activations.relu(self.hidden1(x))\n",
        "        x = keras.activations.relu(self.hidden2(x))\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "    def call(self, x):\n",
        "        x = keras.activations.relu(self.hidden1(x))\n",
        "        x = keras.activations.relu(self.hidden2(x))\n",
        "        x = self.out(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "t_lt6Yf2GHED"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training implementation\n",
        "> *This step defines the functions used in the training process.*"
      ],
      "metadata": {
        "id": "_mN5aU47G_PV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(pred_y, y):\n",
        "  return keras_backend.mean(keras.losses.mean_squared_error(y, pred_y))\n",
        "\n",
        "def np_to_tensor(list_of_numpy_objs):\n",
        "    return (tf.convert_to_tensor(obj) for obj in list_of_numpy_objs)\n",
        "    \n",
        "\n",
        "def compute_loss(model, x, y, loss_fn=loss_function):\n",
        "    logits = model.forward(x)\n",
        "    mse = loss_fn(y, logits)\n",
        "    return mse, logits\n",
        "\n",
        "\n",
        "def compute_gradients(model, x, y, loss_fn=loss_function):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss, _ = compute_loss(model, x, y, loss_fn)\n",
        "    return tape.gradient(loss, model.trainable_variables), loss\n",
        "\n",
        "\n",
        "def apply_gradients(optimizer, gradients, variables):\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    \n",
        "def train_batch(x, y, model, optimizer):\n",
        "    tensor_x, tensor_y = np_to_tensor((x, y))\n",
        "    gradients, loss = compute_gradients(model, tensor_x, tensor_y)\n",
        "    apply_gradients(optimizer, gradients, model.trainable_variables)\n",
        "    return loss\n",
        "\n",
        "def generate_dataset(K, train_size=60, test_size=10):\n",
        "\n",
        "    def _generate_dataset_train(size):\n",
        "        return [YieldGenerator(K=K,list_used=list_train,data=data_all) for _ in range(size)]\n",
        "\n",
        "    return _generate_dataset_train(train_size) \n",
        "\n",
        "class YieldGenerator():\n",
        "\n",
        "    def __init__(self, K=10, list_used = list_train, data = data_all):\n",
        "        self.K = K\n",
        "        self.data = data_all\n",
        "        self.list_used = list_used\n",
        "    \n",
        "\n",
        "    def batch(self, x = None, force_new=False):\n",
        "        number = random.choice(list_train)\n",
        "        data_used = train_all[train_all[reagent] == number]\n",
        "        data_used = data_used.sample(self.K)\n",
        "        x = data_used.iloc[:,0:feature_num].values\n",
        "        y = data_used[['yield']].values\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "xJegncgEHAEt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training model\n",
        "> *This step defines the direct training function.*"
      ],
      "metadata": {
        "id": "ZdyJtIQ-HHm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_maml(model, epochs, dataset, lr_inner=0.00001, batch_size=1, log_steps=1000):\n",
        "\n",
        "    optimizer = keras.optimizers.Adam()\n",
        "    \n",
        "    # Step 2: instead of checking for convergence, we train for a number\n",
        "    # of epochs\n",
        "    for _ in range(epochs):\n",
        "        total_loss = 0\n",
        "        losses = []\n",
        "        losses_test = []\n",
        "        start = time.time()\n",
        "        # Step 3 and 4\n",
        "        for i, t in enumerate(random.sample(dataset, len(dataset))):\n",
        "            x, y = np_to_tensor(t.batch())\n",
        "            model.forward(x)  # run forward pass to initialize weights\n",
        "            with tf.GradientTape() as test_tape:\n",
        "                # test_tape.watch(model.trainable_variables)\n",
        "                # Step 5\n",
        "                with tf.GradientTape() as train_tape:\n",
        "                    train_loss, _ = compute_loss(model, x, y)\n",
        "                # Step 6\n",
        "                gradients = train_tape.gradient(train_loss, model.trainable_variables)\n",
        "                k = 0\n",
        "                model_copy = copy_model(model, x)\n",
        "                for j in range(len(model_copy.layers)):\n",
        "                    model_copy.layers[j].kernel = tf.subtract(model.layers[j].kernel,\n",
        "                                tf.multiply(lr_inner, gradients[k]))\n",
        "                    model_copy.layers[j].bias = tf.subtract(model.layers[j].bias,\n",
        "                                tf.multiply(lr_inner, gradients[k+1]))\n",
        "                    k += 2\n",
        "                # Step 8\n",
        "                test_loss, logits = compute_loss(model_copy, x, y)\n",
        "\n",
        "            # Step 8\n",
        "            gradients = test_tape.gradient(test_loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "            \n",
        "            # Logs\n",
        "            total_loss += test_loss\n",
        "            loss = total_loss / (i+1.0)\n",
        "            losses.append(loss)\n",
        "            \n",
        "\n",
        "def copy_model(model, x):\n",
        "\n",
        "    copied_model = Model()\n",
        "    copied_model.forward(tf.convert_to_tensor(x))\n",
        "    \n",
        "    copied_model.set_weights(model.get_weights())\n",
        "    return copied_model"
      ],
      "metadata": {
        "id": "Mf20cAssHQlM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Validation, model selection and model saving\n",
        "> *This step defines the functions used to validate the model. It also includes the code of model selection and model saving.*"
      ],
      "metadata": {
        "id": "4mSDf83IHnsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_all_ks_tune_num_val(model,tune_num=10):\n",
        "\n",
        "  y_test_all = []\n",
        "  y_pred_all = []\n",
        "\n",
        "  for i in range(len(list_val)):\n",
        "    y_test,y_pred,df1,df2,df_plot = eval_ks_tune_num(model,number = list_val[i],tune_num=tune_num)\n",
        "\n",
        "    y_test_all += y_test\n",
        "    y_pred_all += y_pred\n",
        "\n",
        "  r2 = metrics.r2_score(y_test_all, y_pred_all)\n",
        "  rmse = np.sqrt(metrics.mean_squared_error(y_test_all, y_pred_all))\n",
        "\n",
        "  return r2,rmse\n",
        "\n",
        "def eval_ks_tune_num(model, data = data_all, num_steps=(0, 1, 2,3,4,5,6,7,8,9,10), lr=0.00001, number = 16,tune_num=10):\n",
        "\n",
        "    df1 = pd.DataFrame()\n",
        "    df2 = pd.DataFrame()\n",
        "    df_plot = pd.DataFrame()\n",
        "    tmp = data_all_copy[data_all_copy[reagent] == number]\n",
        "\n",
        "    data_used = tmp.reset_index().drop(columns=['index'])\n",
        "    data_used_plot = tmp.reset_index()\n",
        "\n",
        "    X = data_used.iloc[:,feature_num+2:feature_num+4]\n",
        "    y = data_used.iloc[:,feature_num+1:feature_num+2]\n",
        "\n",
        "    all_num = len(data_used)\n",
        "\n",
        "    X_train_, X_test_, y_train_, y_test_ = ks.train_test_split(X, y, test_size = 1 - tune_num/all_num)\n",
        "    top_k_idx = X_train_.index.tolist()\n",
        "\n",
        "    data_sampled = data_used.loc[top_k_idx]\n",
        "    data_sampled_plot = data_used_plot.loc[top_k_idx].set_index([\"index\"])\n",
        "\n",
        "    # batch used for training\n",
        "    data_minus = data_used.append(data_sampled).drop_duplicates(keep=False)\n",
        "\n",
        "    x_test = data_minus.iloc[:,0:feature_num].values\n",
        "    y_test = data_minus[['yield']].values.flatten().tolist()\n",
        "    \n",
        "    x = data_sampled.iloc[:,0:feature_num].values\n",
        "    y = data_sampled[['yield']].values\n",
        "\n",
        "    \n",
        "    # copy model so we can use the same model multiple times\n",
        "    copied_model = copy_model(model, x)\n",
        "    \n",
        "    # use SGD for this part of training as described in the paper\n",
        "    optimizer = keras.optimizers.SGD(learning_rate=lr)\n",
        "    \n",
        "    # run training and log fit results\n",
        "    fit_res,best_res = evaluation(copied_model, optimizer, x, y, x_test, y_test, num_steps)\n",
        "    \n",
        "    #y_pred = np.clip(best_res[0][1].numpy().flatten(), 0, 100)\n",
        "    #y_pred = best_res[0][1].numpy().flatten()\n",
        "    y_pred = fit_res[1][1].numpy().flatten().tolist()\n",
        "\n",
        "    \n",
        "    df1 = data_sampled\n",
        "    df2 = data_minus\n",
        "    df_plot = data_sampled_plot\n",
        "\n",
        "    return y_test,y_pred,df1,df2,df_plot\n",
        "\n",
        "def evaluation(model, optimizer, x, y, x_test, y_test, num_steps=(0, 1, 2,3,4,5,6,7,8,9,10)):\n",
        "\n",
        "    fit_res = []\n",
        "    best_res = []\n",
        "    min_loss = 1000000000\n",
        "    \n",
        "    tensor_x_test, tensor_y_test = np_to_tensor((x_test, y_test))\n",
        "    \n",
        "    # If 0 in fits we log the loss before any training\n",
        "    if 0 in num_steps:\n",
        "        loss, logits = compute_loss(model, tensor_x_test, tensor_y_test)\n",
        "        fit_res.append((0, logits, loss))\n",
        "        \n",
        "    for step in range(1, np.max(num_steps) + 1):\n",
        "        train_batch(x, y, model, optimizer)\n",
        "        loss, logits = compute_loss(model, tensor_x_test, tensor_y_test)\n",
        "        if step in num_steps:\n",
        "            fit_res.append(\n",
        "                (\n",
        "                    step, \n",
        "                    logits,\n",
        "                    loss\n",
        "                )\n",
        "            )\n",
        "\n",
        "        if loss < min_loss:\n",
        "          best_res = []\n",
        "          best_res.append(\n",
        "                (\n",
        "                    step, \n",
        "                    logits,\n",
        "                    loss\n",
        "                )\n",
        "            )\n",
        "          min_loss = loss\n",
        "    \n",
        "    return fit_res,best_res\n",
        "\n",
        "val_times = 3\n",
        "feature_num = 320\n",
        "result = pd.DataFrame()\n",
        "train_ds = generate_dataset(K=20,train_size=80)\n",
        "\n",
        "# model selection with validation set\n",
        "model_list = []\n",
        "for i in range(val_times): \n",
        "  maml = Model()  \n",
        "  train_maml(maml, 20, train_ds, log_steps=10)\n",
        "  model_list.append(maml)\n",
        "  r2,rmse = eval_all_ks_tune_num_val(maml,tune_num = 5)\n",
        "\n",
        "  c={\"r2_val\" : [r2]}\n",
        "  frame = pd.DataFrame(c)\n",
        "  frame['val_times'] = i\n",
        "  result = result.append(frame)\n",
        "\n",
        "tmp = result[result['r2_val']==result['r2_val'].max()]\n",
        "#model_list[tmp.at[0,'val_times']].save_weights(output_path) # Save the model"
      ],
      "metadata": {
        "id": "S4hAFWfQGIRk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model fine-tuning and testing"
      ],
      "metadata": {
        "id": "qYJll48FI7Jl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading model\n",
        "> *This step loads the trained model.*"
      ],
      "metadata": {
        "id": "Dh2zuEzEJAmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = os.path.join(BASE_DIR, 'model/model_trained.h5')\n",
        "\n",
        "maml_saved = Model()\n",
        "maml_saved.build(input_shape = (320,320))\n",
        "maml_saved.load_weights(model_path)"
      ],
      "metadata": {
        "id": "PqrZruNcJupS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fine-tuning and printing experiment results\n",
        "> *This step performs model fine-tuning and prints out experiment results.*"
      ],
      "metadata": {
        "id": "gR3Ohf4-LGIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_all_ks_tune_num(model,tune_num=10):\n",
        "\n",
        "  y_test_all = []\n",
        "  y_pred_all = []\n",
        "\n",
        "  for i in range(len(list_test)):\n",
        "    y_test,y_pred,df1,df2,df_plot = eval_ks_tune_num(model,number = list_test[i],tune_num=tune_num)\n",
        "\n",
        "    y_test_all += y_test\n",
        "    y_pred_all += y_pred\n",
        "\n",
        "  r2 = metrics.r2_score(y_test_all, y_pred_all)\n",
        "  rmse = np.sqrt(metrics.mean_squared_error(y_test_all, y_pred_all))\n",
        "  print(\"r2:\",r2)\n",
        "  print(\"rmse:\",rmse)"
      ],
      "metadata": {
        "id": "v5QYmOaVQDqU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_all_ks_tune_num(maml_saved,tune_num=5)   "
      ],
      "metadata": {
        "id": "1VGitId_LNaC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}